# .github/workflows/db-backup.yml
name: Daily Database Backup (Render Postgres -> R2)

on:
  schedule:
    - cron: "0 1 * * *"
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Install PostgreSQL 18 client
        run: |
          set -euo pipefail
          sudo apt-get update
          sudo apt-get install -y wget gnupg lsb-release ca-certificates
          wget -qO - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" | sudo tee /etc/apt/sources.list.d/pgdg.list
          sudo apt-get update
          sudo apt-get install -y postgresql-client-18

      - name: Create database dump
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          set -euo pipefail
          TS="$(date -u +%Y-%m-%d_%H-%M-%S)"
          FILE="backup_${TS}.dump"

          # Explicitly use PostgreSQL 18 pg_dump binary
          /usr/lib/postgresql/18/bin/pg_dump "$DATABASE_URL" -F c -f "$FILE"

          echo "BACKUP_FILE=$FILE" >> $GITHUB_ENV

      - name: Install AWS CLI
        run: |
          set -euo pipefail
          curl -sS "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip -q awscliv2.zip
          sudo ./aws/install

      - name: Upload backup to Cloudflare R2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          set -euo pipefail
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          aws s3 cp "${BACKUP_FILE}" "s3://${R2_BUCKET}/${BACKUP_FILE}" --endpoint-url "$ENDPOINT"

      - name: Keep only 7 newest backups
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
        run: |
          set -euo pipefail
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          aws s3 ls "s3://${R2_BUCKET}/" --endpoint-url "$ENDPOINT" \
            | awk '{print $4}' \
            | sort \
            | head -n -7 \
            | while read -r key; do
                [ -z "$key" ] && continue
                aws s3 rm "s3://${R2_BUCKET}/${key}" --endpoint-url "$ENDPOINT"
              done
